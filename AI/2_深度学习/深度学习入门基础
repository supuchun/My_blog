1、机器学习的子类--深度学习
    深度学习框架：
        TensorFlow：google开源的，当前版本已经有高级API、可视化工具、GPU支持、异步执行
        DeepLearning4J：（DL4J）是一套基于Java语言的神经网络工具包，可以构建、定型和部署神经网络。DL4J与Hadoop和Spark集成，支持分布式CPU和GPU。
        Keras:Keras是一个高层神经网络API，Keras由纯Python编写而成并基Tensorflow或Theano。Keras为支持快速实验而生，能够把你的idea迅速转换为结果，
        如果你有如下需求，请选择Keras：简易和快速的原型设计（keras具有高度模块化，极简，和可扩充特性），支持CNN和RNN，或二者的结合。

2、 CNN，卷积神经网络主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。
    RNN，循环神经网络训练样本输入是连续的序列，且序列的长短不一，比如基于时间的序列：一段连续的语音，一段连续的手写文字。这些序列比较长，
    且长度不一，比较难直接的拆分成一个个独立的样本来通过DNN/CNN进行训练。
    BP，反向传播算法

3、deep learning训练过程具体如下：
	1）使用自下上升非监督学习（就是从底层开始，一层一层的往顶层训练）：
       采用无标定数据（有标定数据也可）分层训练各层参数，这一步可以看作是一个无监督训练过程，是和传统神经网络区别最大的部分（这个过程可以看作是feature learning过程）：
       具体的，先用无标定数据训练第一层，训练时先学习第一层的参数（这一层可以看作是得到一个使得输出和输入差别最小的三层神经网络的隐层），由于
       模型capacity的限制以及稀疏性约束，使得得到的模型能够学习到数据本身的结构，从而得到比输入更具有表示能力的特征；在学习得到第n-1层后，将n-1层的输出作为第n层的输入，训练第n层，由此分别得到各层的参数；
	2）自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）
       基于第一步得到的各层参数进一步fine-tune整个多层模型的参数，这一步是一个有监督训练过程；第一步类似神经网络的随机初始化初值过程，由于DL的第一步不是随机初始化，而是通过学习输入数据的结构得到的，
       因而这个初值更接近全局最优，从而能够取得更好的效果；所以deep learning效果好很大程度上归功于第一步的feature learning过程。

4、神经网络的训练一般采用反向传播算法+梯度下降法。反向传播算法从复合函数求导的链式法则导出，因为神经网络是一个多层的复合函数。反向传
    播算法计算误差项时每一层都要乘以本层激活函数的导数。sigmoid函数的输出映射在(
    0,1)之间，单调连续，求导容易。但是由于其软饱和性，容易产生梯度消失，导致训练出现问题；另外它的输出并不是以0为中心的。
    tanh函数的输出值以0为中心，位于(-1,+1)区间，相比sigmoid函数训练时收敛速度更快，但它还是饱和函数，存在梯度消失问题。
    ReLU函数其形状为一条折线，当x<0时做截断处理。该函数在0点出不可导，如果忽略这一个点其导数为sgn。函数的导数计算很简单，而且由于在正半轴导数为1，有效的缓解了梯度消失问题。在ReLU的基础上又出现了各种新的激活函数，包括ELU、PReLU等。
    在神经网络的早期阶段，sigmoid函数，tanh被广为使用。在AlexNet出现之后，ReLU函数逐渐取代了这两个函数，得到了广泛使用，因为ReLU函数更不容易产生梯度消失问题。
    相对于sigmoid和tanh激励函数，对ReLU求梯度非常简单，计算也很简单，可以非常大程度地提升随机梯度下降的收敛速度。（因为ReLU是线性的，而sigmoid和tanh是非线性的）。
    在Caffe中，激活函数是一个单独的层，把它和全连接层，卷据层拆开的好处是更为灵活，便于代码复用和组合。

5、卷积神经网络在本质上也是一个多层复合函数，但和普通神经网络不同的是它的某些权重参数是共享的，另外一个特点是它使用了池化层。训练时
    依然采用了反向传播算法，求解的问题不是凸优化问题。核心：一个共享权重的多层复合函数。
    权值共享网络结构使之更类似于生物神经网络，降低了网络模型的复杂度，减少了权值的数量。该优点在网络的输入是多维图像时表现的更为明显，使图像可以直接作为网络的输入，避免了传统识别算法中复杂的特征提取和数据重建过程。
    卷积网络是为识别二维形状而特殊设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性。
    循环神经网络是一个递推的数列，因此具有了记忆功能。核心：综合了复合函数和递推数列的一个函数。
    k均值算法是一种无监督的聚类算法。算法将每个样本分配到离它最近的那个类中心所代表的类，而类中心的确定又依赖于样本的分配方案。这是一个先有鸡还是先有蛋的问题。核心：把样本分配到离它最近的类中心所属的类，类中心由属于这个类的所有样本确定。

6、偏差的目的是来改变权重与输入相乘所得结果的范围的。通常在卷积层之间定期引入池层。这基本上是为了减少一些参数，并防止过度拟合。

7、池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。如果输入是图像的话，那么池化层的最主要作用就是压缩图像。

8、模型优化
    深度学习的神经网络的训练一般采用反向传播算法+梯度下降法。
    目标函数=损失函数/代价函数+正则化项：通过交叉熵获取训练方法（即最优化算法）
        0.反向传播算法:back propagation:BP

        1.梯度下降法:gradient descent:GD
            梯度下降法沿着梯度的反方向进行搜索，利用了函数的一阶导数信息。根据函数的一阶泰勒展开，在负梯度方向，函数值是下降的。只要学习率设置的足够小，并且没有到达梯度为0的点处，每次迭代时函数值一定会下降。梯度下降法只能保证找到梯度为0的点，不能保证找到极小值点。迭代终止的判定依据是梯度值充分接近于0，或者达到最大指定迭代次数。

        2.凸优化:convex optimization:CO
            优化变量的可行域是一个凸集，目标函数是一个凸函数。凸优化最好的一个性质是：所有局部最优解一定是全局最优解。
        3.牛顿法:newton method:NM
            牛顿法利用了函数的一阶和二阶导数信息，直接寻找梯度为0的点。牛顿法不能保证每次迭代时函数值下降，也不能保证收敛到极小值点。在实现时，也需要设置学习率，原因和梯度下降法相同，是为了能够忽略泰勒展开中的高阶项。学习率的设置通常采用直线搜索（line search）技术。迭代终止的判定依据是梯度值充分接近于0，或者达到最大指定迭代次数。牛顿法比梯度下降法有更快的收敛速度，但每次迭代时需要计算Hessian矩阵，并求解一个线性方程组，运算量大。

        4.自适应矩估计法:adaptive moment estimation:Adam
            Adam 算法根据损失函数对每个参数的梯度的一阶矩估计和二阶矩估计动态调整针对于每个参数的学习速率。TensorFlow提供的tf.train.AdamOptimizer可控制学习速度。Adam 也是基于梯度下降的方法，但是每次迭代参数的学习步长都有一个确定的范围，不会因为很大的梯度导致很大的学习步长，参数的值比较稳定。AdamOptimizer通过使用动量（参数的移动平均数）来改善传统梯度下降，促进超参数动态调整。
            自适应优化算法通常都会得到比SGD算法性能更差（经常是差很多）的结果，尽管自适应优化算法在训练时会表现的比较好，因此使用者在使用自适应优化算法时需要慎重考虑。

        5.随机梯度下降法:Stochastic Gradient Descent:SGD
             要优化一个函数 f(x)，即找到它的最小值, 常用的方法叫做Gradient Descent (GD), 也就是最速下降法。
             SGD一般都指mini-batch gradient descent，即随机抽取一批样本,以此为根据来更新参数。

9、机器视觉研究方向
    通用目标检测
    行人检测
    人脸检测
    人脸识别
    通用图像分类
    光学字符识别（OCR）
    各种图像处理算法，如分割、增强等
    目标跟踪
    3D视觉
    医学图像分析
    车牌识别