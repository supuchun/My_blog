机器学习算法
    监督学习
        1、回归算法：线性回归和逻辑回归。
                线性回归：进行直线或曲线拟合，一般使用“最小二乘法”来求解。最小二乘法将最优问题转化为求函数极值问题。函数极值在数学上我们一般会采用求导数为0的方法。
                        但这种做法并不适合计算机，可能求解不出来，也可能计算量太大。计算机科学界专门有一个学科叫“数值计算”，专门用来提升计算机进行各类计算时的准确性和效率问题。
                        例如，著名的“梯度下降”以及“牛顿法”就是数值计算中的经典算法，也非常适合来处理求解函数极值的问题。梯度下降法是解决回归模型中最简单且有效的方法之一。
                        从严格意义上来说，由于后文中的神经网络和推荐算法中都有线性回归的因子，因此梯度下降法在后面的算法实现中也有应用。
                逻辑回归：线性回归处理的是数值问题，也就是最后预测出的结果是数字。而逻辑回归属于分类算法，也就是说，逻辑回归预测结果是离散的分类，例如判断这封邮件是否是垃圾邮件，
                        以及用户是否会点击此广告等等。实现方面的话，逻辑回归只是对对线性回归的计算结果加上了一个Sigmoid函数，将数值结果转化为了0到1之间的概率(Sigmoid函数的图像一般来说并不直观，你只需要理解对数值越大，
                        函数越逼近1，数值越小，函数越逼近0)，接着我们根据这个概率可以做预测，例如概率大于0.5，则这封邮件就是垃圾邮件，或者肿瘤是否是恶性的等等
                        下面的两个算法是机器学习界最强大且重要的算法，都可以拟合出非线性的分类线。logistic回归核心：直接从样本估计出它属于正负样本的概率。通过先将向量进行线性加权，
                        然后计算logistic函数，可以得到[0,1]之间的概率值，它表示样本x属于正样本的概率，正样本标签值为1，负样本为0。训练时，求解的的是对数似然函数。
                        这是一个凸优化问题，求解时可以用梯度下降法，也可以用牛顿法。logistic回归是一种判别模型，需要注意的是它是一种线性模型，用于二分类问题。
        2、贝叶斯分类器直接用贝叶斯公式解决分类问题 ，是一种生成模型，处理多分类问题，是一种非线性模型。
        3、决策树在本质上是一组嵌套的if-else判定规则，从数学上看是分段常数函数，对应于用平行于坐标轴的平面对空间的划分。决策树是一种判别模型，既支持分类问题，也支持回归问题，是一种非线性模型（分段线性函数不是线性的）。它天然的支持多分类问题。
        4、随机森林核心：用有放回采样的样本训练多棵决策树，训练决策树的每个节点是只用了无放回抽样的部分特征，预测时用这些树的预测结果进行投票。随机森林是一种集成学习算法，它由多棵决策树组成。
            对于分类问题，一个测试样本会送到每一棵决策树中进行预测，然后投票，得票最多的类为最终分类结果。对于回归问题随机森林的预测输出是所有决策树输出的均值。
            随机森林是一种判别模型，既支持分类问题，也支持回归问题，并且支持多分类问题。这是一种非线性模型。
        5、kNN（K-近邻算法）本质上使用了模板匹配的思想。要确定一个样本的类别，可以计算它与所有训练样本的距离，然后找出和该样本最接近的k个样本，统计这些样本的类别进行投票，票数最多的那个类就是分类结果。
            kNN算法是一种判别模型，即支持分类问题，也支持回归问题，是一种非线性模型。它天然的支持多分类问题。kNN算法没有训练过程，是一种基于实例的算法。
        6、支持向量机的目标是寻找一个分类超平面，它不仅能正确的分类每一个样本，并且要使得每一类样本中距离超平面最近的样本到超平面的距离尽可能远。如果不使用非线性核函数，SVM是一个线性模型。
            训练时求解的问题是凸优化问题，求解采用了SMO算法，这是一种分治法，每次挑选出两个变量进行优化，其他变量保持不动。选择优化变量的依据是KKT条件，对这两个变量的优化是一个二次函数极值问题，
            可以直接得到公式解。SVM是一种判别模型。它既可以用于分类问题，也可以用于回归问题。标准的SVM只能支持二分类问题，使用多个分类器的组合，可以解决多分类问题。
    无监督学习
        7、K均值算法核心：把样本分配到离它最近的类中心所属的类，类中心由属于这个类的所有样本确定。k均值算法是一种无监督的聚类算法。算法将每个样本分配到离它最近的那个类中心所代表的类，而类中心的确定又依赖于样本的分配方案。这是一个先有鸡还是先有蛋的问题。
            在实现时，先随机初始化每个类的类中心，然后计算样本与每个类的中心的距离，将其分配到最近的那个类，然后根据这种分配方案重新计算每个类的中心。这也是一种分阶段优化的策略。
            k均值算法要求解的问题是一个NPC问题，只能近似求解，有陷入局部极小值的风险。

深度学习算法
    神经网络：监督学习
        8、人工神经网络在本质上是一个多层的复合函数，它实现了从向量x到向量y的映射。由于使用了非线性的激活函数f，这个函数是一个非线性函数。神经网络训练时求解的问题不是凸优化问题。反向传播算法由多元复合函数求导的链式法则导出。
            标准的神经网络是一种有监督的学习算法，是一种非线性模型，它既可以用于分类问题，也可以用于回归问题，天然的支持多分类问题。
        9、卷积神经网络核心（CNN）：一个共享权重的多层复合函数。卷积神经网络在本质上也是一个多层复合函数，但和普通神经网络不同的是它的某些权重参数是共享的，另外一个特点是它使用了池化层。训练时依然采用了反向传播算法，求解的问题不是凸优化问题。
            和全连接神经网络一样，卷积神经网络是一个判别模型，它既可以用于分类问题，也可以用用于回归问题，并且支持多分类问题。
        10、循环神经网络核心（RNN）：综合了复合函数和递推数列的一个函数。和普通神经网络最大的不同在于，循环神经网络是一个递推的数列，因此具有了记忆功能。
            和其他类型的神经网络一样，循环神经网络是一个判别模型，既支持分类问题，也支持回归问题，并且支持多分类问题

大数据
    数据挖掘&数据分析
        推荐算法

一些算法本身并不算是一个机器学习算法，而是为了解决某个子问题而诞生的机器学习算法的子算法，如深度学习的神经网络的训练一般采用反向传播算法+梯度下降法
    梯度下降法：主要运用在线型回归，逻辑回归，神经网络，推荐算法中。
    BP算法：主要运用在神经网络中
    牛顿法：主要运用在线型回归中。
    SMO算法：主要运用在SVM中
    


