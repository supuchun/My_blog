1、机器学习的子类--深度学习
    深度学习框架：
        TensorFlow：google开源的，当前版本已经有高级API、可视化工具、GPU支持、异步执行
        DeepLearning4J：（DL4J）是一套基于Java语言的神经网络工具包，可以构建、定型和部署神经网络。DL4J与Hadoop和Spark集成，支持分布式CPU和GPU。
        Keras:Keras是一个高层神经网络API，Keras由纯Python编写而成并基Tensorflow或Theano。Keras为支持快速实验而生，能够把你的idea迅速转换为结果，
        如果你有如下需求，请选择Keras：简易和快速的原型设计（keras具有高度模块化，极简，和可扩充特性），支持CNN和RNN，或二者的结合
2、 CNN，卷积神经网络主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。
    RNN，循环神经网络训练样本输入是连续的序列，且序列的长短不一，比如基于时间的序列：一段连续的语音，一段连续的手写文字。这些序列比较长，
    且长度不一，比较难直接的拆分成一个个独立的样本来通过DNN/CNN进行训练。
    BP，反向传播算法
3、deep learning训练过程具体如下：
	1）使用自下上升非监督学习（就是从底层开始，一层一层的往顶层训练）：
	       采用无标定数据（有标定数据也可）分层训练各层参数，这一步可以看作是一个无监督训练过程，是和传统神经网络区别最大的部分（这个过程可以看作是feature learning过程）：
	       具体的，先用无标定数据训练第一层，训练时先学习第一层的参数（这一层可以看作是得到一个使得输出和输入差别最小的三层神经网络的隐层），由于
	       模型capacity的限制以及稀疏性约束，使得得到的模型能够学习到数据本身的结构，从而得到比输入更具有表示能力的特征；在学习得到第n-1层后，将n-1层的输出作为第n层的输入，训练第n层，由此分别得到各层的参数；
	2）自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）
	       基于第一步得到的各层参数进一步fine-tune整个多层模型的参数，这一步是一个有监督训练过程；第一步类似神经网络的随机初始化初值过程，由于DL的第一步不是随机初始化，而是通过学习输入数据的结构得到的，
           因而这个初值更接近全局最优，从而能够取得更好的效果；所以deep learning效果好很大程度上归功于第一步的feature learning过程。
4、神经网络的训练一般采用反向传播算法+梯度下降法。反向传播算法从复合函数求导的链式法则导出，因为神经网络是一个多层的复合函数。反向传播算法计算误差项时每一层都要乘以本层激活函数的导数。
    sigmoid函数的输出映射在(0,1)之间，单调连续，求导容易。但是由于其软饱和性，容易产生梯度消失，导致训练出现问题；另外它的输出并不是以0为中心的。
    tanh函数的输出值以0为中心，位于(-1,+1)区间，相比sigmoid函数训练时收敛速度更快，但它还是饱和函数，存在梯度消失问题。
    ReLU函数其形状为一条折线，当x<0时做截断处理。该函数在0点出不可导，如果忽略这一个点其导数为sgn。函数的导数计算很简单，而且由于在正半轴导数为1，有效的缓解了梯度消失问题。在ReLU的基础上又出现了各种新的激活函数，包括ELU、PReLU等。
    在神经网络的早期阶段，sigmoid函数，tanh被广为使用。在AlexNet出现之后，ReLU函数逐渐取代了这两个函数，得到了广泛使用，因为ReLU函数更不容易产生梯度消失问题。
    相对于sigmoid和tanh激励函数，对ReLU求梯度非常简单，计算也很简单，可以非常大程度地提升随机梯度下降的收敛速度。（因为ReLU是线性的，而sigmoid和tanh是非线性的）。
    在Caffe中，激活函数是一个单独的层，把它和全连接层，卷据层拆开的好处是更为灵活，便于代码复用和组合。
5、卷积神经网络在本质上也是一个多层复合函数，但和普通神经网络不同的是它的某些权重参数是共享的，另外一个特点是它使用了池化层。训练时依然采用了反向传播算法，求解的问题不是凸优化问题。核心：一个共享权重的多层复合函数。
    权值共享网络结构使之更类似于生物神经网络，降低了网络模型的复杂度，减少了权值的数量。该优点在网络的输入是多维图像时表现的更为明显，使图像可以直接作为网络的输入，避免了传统识别算法中复杂的特征提取和数据重建过程。
    卷积网络是为识别二维形状而特殊设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性。
    循环神经网络是一个递推的数列，因此具有了记忆功能。核心：综合了复合函数和递推数列的一个函数。
    k均值算法是一种无监督的聚类算法。算法将每个样本分配到离它最近的那个类中心所代表的类，而类中心的确定又依赖于样本的分配方案。这是一个先有鸡还是先有蛋的问题。核心：把样本分配到离它最近的类中心所属的类，类中心由属于这个类的所有样本确定。
6、偏差的目的是来改变权重与输入相乘所得结果的范围的。通常在卷积层之间定期引入池层。这基本上是为了减少一些参数，并防止过度拟合。
