1：目前的自动驾驶系统由摄像机，激光雷达等传感器，控制器，GPS定位系统，数字地图，算法等多个部件构成，算法部分，尤其是机器学习深度学习技术在其中是重点。
第一步：行驶的起始地和目的地，计算最优的行驶路径
    1、起始地和目的的定位问题，可以通过GPS或其他技术手段来实现
    2、路径规划问题， 计算出一条最优的行驶路径，可以通过成熟的算法，如Dijkstra或者A*搜索算法实现，它给出了计算图的两个节点之间最短距离的方案。目前，这一问题已经很好的解决了，
       而且计算机比人要强。
第二步：启动汽车开始行驶
    1、机器学习登场了，它要解决路面和车道线检测问题。目前主流的自动驾驶系统一般都采用了激光雷达+摄像机+其他传感器相结合的方案。无论是激光雷达扫描得到的3D距离数据，还是摄像机成像的2D数据，
       我们都要对它们进行分析，以准确的确定路面的位置，车道线和每个车道的范围。
    2、路上有没有车，有没有人，有多少车，有多少人，以及其他障碍物，它们在路面的什么地方。这又是机器学习和机器视觉要解决的问题，同样是检测问题。我们需要对激光雷达或者摄像机的图像进行分析，
       得到这些障碍物的准确位置。
    3、行驶过程中，你遇到的这些行人，车辆都是移动的，因此你必须要对他们的运动趋势做出预判。你前面的车辆、后面的车辆的行驶速度和轨迹都会影响你要采取的动作。这是机器视觉中的目标跟踪问题，
       我们要准确的跟踪出人，车辆，动物等移动目标的运动轨迹，估计出他们的运动速度与方向，以便于做出决策。
    4、行驶一会儿之后，你遇到了第一个十字路口，这里有红绿灯，当前是红灯，因此你需要停下来等待，而不是硬闯过去，这又涉及到一个问题，怎么知道这些交通灯？
       这依然是机器视觉要解决的问题，即准确的检测出图像中的交通灯，并知道它们当前的状态。除了红绿灯之外，还有其他交通标志需要我们识别，比如速度限制、是否允许调头等。
    5、还有一个问题没有解决，在知道这些环境参数之后，该怎么行驶？即根据环境参数得到要执行的动作，在这里是车辆行驶的速度（速度是一个矢量，具有大小和方向）。最简单的做法是用规则来做决策，
       总结出人驾驶车辆的经验，前面没有车，后面没有车的时候该怎么行驶；前面有2辆车，后面有3辆车的时候该怎么行驶.....。问题是：各种情况实在是太多了，我们无法穷举出所有的情况。
       对于这个问题的一个解决方案是深度强化学习，和AlphaGo类似的一种技术，这也是一种机器学习算法。它的思路是根据环境的参数预测出要执行的动作，我们用一些数据进行训练，得到这样一个模型，
       也就是人开车时的经验，然后用它来做决策。但是这种方法有一个严重的问题，神经网络的预测结果不具有可解释性，有时候会出现莫名其妙的结果，这会严重影响安全性。关注过AlphaGo的同学都知道，
       在一次对战中，它下出了一个完全无法理解的棋，对于自动驾驶来说，这可能是一个灾难。
    
2：百度阿波罗平台简介
    阿波罗的官网地址是：http://apollo.auto/
    源代码，文档与数据下载地址为：https://github.com/apolloauto
    阿波罗官方对目前状态的整体介绍：
    阿波罗2.5版本的目标是用低成本的传感器实现自动驾驶。它能让车辆保持在某一车道上，并与前面最近的车辆保持距离，这通过一个前视摄像头，以及前视雷达来实现。对摄像头图像的分析采用了深度神经网络，
    随着样本数据的累积，神经网络的预测将越来越准。官方说明，目前不支持在高度弯曲，没有车道线标志的道路上行驶。
    在这里，我们重点关注的是感知模块，感知模块为我们提供了类似人类眼睛所提供的视觉功能，即理解我们所处的驾驶环境。首先来看阿波罗官方对感知模块的介绍（以下两段话引用了他们的原文）：
    “Apollo 2.0感知模块包括障碍物检测识别和红绿灯检测识别两部分。障碍物检测识别模块通过输入激光雷达点云数据和毫米波雷达数据，输出基于两种传感器的障碍物融合结果，包括障碍物的位置、
    形状、类别、速度、朝向等信息。红绿灯检测识别模块通过输入两种焦距下的相机图像数据，输出红绿灯的位置、颜色状态等信息。上述两大感知功能，使无人车具备在简单城市道路自动驾驶的能力
    “通过安装在车身的各类传感器如激光雷达、摄像头和毫米波雷达等获取车辆周边的环境数据。利用多传感器融合技术，车端感知算法能够实时计算出环境中交通参与者的位置、类别和速度朝向等信息。
    背后支持这套自动驾驶感知系统的是多年积累的大数据和深度学习技术，海量的真实路测数据经过专业人员的标注变成机器能够理解的学习样本，大规模深度学习平台和GPU集群将离线学习大量数据所耗费的时间大幅缩短，
    训练好的最新模型通过在线更新的方式从云端更新到车载大脑。人工智能+数据驱动的解决方案使百度无人车感知系统能够持续不断的提升检测识别能力，为自动驾驶的决策规划控制模块提供准确、稳定、可靠的输入。”
    从这里可以看到，他们采用了摄像机，激光雷达，毫米波雷达等多种传感器，用深度学习技术对这些传感器采集的数据进行分析，以确定车辆当前所处环境中的交通参与者，这里的参与者是指人，车等重要目标。
    核心的算法包括：
        车道检测
        目标检测
        车道线检测
        目标跟踪
        轨迹管理
        相机标定
        预测算法
        规划算法
    1、道路和车道线识别：
        车道线属于静态目标，不会移动。准确的确定车道线，不仅对车辆的纵向控制有用，还对横向控制有用。车道线由一系列的线段集合来表示。首先，用卷积神经网络对摄像机采集的图像进行处理，
        预测出车道线的概率图，即每一点处是车道线的概率。然后，对这种图进行二值化，得到分割后的二值图像。接下来计算二值图像的联通分量，检测出所有的内轮廓，然后根据轮廓边缘点得到车道的标志点。
        从描述文档看，核心的一步是用卷积神经网络预测出图像每一点是车道线的概率。
    2、障碍物检测识别：
        官方对障碍物 检测模块的描述：“障碍物模块包括基于激光雷达点云数据的障碍物检测识别、基于毫米波雷达数据的障碍物检测识别以及基于两种传感器的障碍物结果融合。
        基于激光雷达点云数据的障碍物检测识别，通过线下训练的卷积神经网络模型，学习点云特征并预测障碍物的相关属性（比如前景物体概率、相对于物体中心的偏移量、物体高度等），并根据这些属性进行障碍物分割。
        基于毫米波雷达数据的障碍物检测识别，主要用来对毫米波雷达原始数据进行处理而得到障碍物结果。该算法主要进行了ID扩展、噪点去除、检测结果构建以及ROI过滤。多传感器障碍物结果融合算法，
        用于将上述两种传感器的障碍物结果进行有效融合。该算法主要进行了单传感器结果和融合结果的管理、匹配以及基于卡尔曼滤波的障碍物速度融合。具体算法请参阅github技术文档。”
        可以看出，对障碍物的检测与车道线检测不同，这里采用的是基于激光雷达和毫米波雷达的数据。这是出于安全的考虑，如果采用摄像机，在恶劣天气如雨雪，以及极端光照条件下，图像将无法有效的分析，
        另外，激光雷达和毫米波雷达给出了物体准确的距离数据，这对安全的行驶至关重要，而单纯靠图像数据分析则很难做到。
        神经网络实现方案的描述：考虑了两种方案：车道线检测和障碍物检测使用同一个神经网络，以及各自使用一个神经网络。后者的准确率更高，但更耗费计算资源，而且处理时间可能会更长。最后选用的是第一种方案。
        具体的，采用了YOLO[1][2]作为基础网络来进行目标和车道检测。目前区分的目标有汽车，自行车，行人。这些目标都用一个2D的矩形框表示，并且带有朝向信息。车道线用同一个网络的输出得到，使用了图像分割技术。
        阿波罗将目标分为两种类型，静态的和动态的，静态的目标包括车道线，交通灯，其他各种写有文字的交通标志。除此之外，路上还有一些标志可用于视觉定位，包括路灯，栅栏，天桥，地平线等。
        动态目标目前关注的是车辆，自行车，行人，动物等。在这些目标中，最重要的是道路上离我们最近的物体，其次是相邻车道上的物体。
    3、目标跟踪：
        在检测出各个运动目标之后，接下来需要准确的跟踪这些目标，得到他们的运动参数和轨迹。目标跟踪是一个状态估计问题，这里的状态就是目标的位置，速度，加速度等参数。跟踪算法可以分为单目标跟踪和多目标跟踪两类，
        前者只跟踪单个目标，后者可以同时跟踪多个目标。跟踪算法的数据来源是目标检测的输出结果，即在每一个时刻先检测出路上的移动目标，得到他们的位置，大小等信息，然后对不同时刻的这些数据进行分析，得到目标的状态和运动轨迹。
        单目标跟踪算法的核心是估计出单个目标的位置，速度，加速度等状态信息，典型的算法有卡尔曼滤波，粒子滤波等。和单个目标跟踪不同，多目标跟踪需要解决数据关联问题，即上一帧的每个目标和下一帧的哪个目标对应，还要解决新目标出现，
        老目标消失问题。多目标的跟踪的一般流程为每一时刻进行目标检测，然后进行数据关联，为已有目标找到当前时刻的新位置，在这里，目标可能会消失，也可能会有新目标出现，另外目标检测结果可能会存在虚警和漏检测。联合概率滤波，
        多假设跟踪，线性规划，全局数据关联，MCMC马尔可夫链蒙特卡洛算法先后被用于解决数据关联问题来完成多个目标的跟踪。首先我们定义多目标跟踪的中的基本概念，目标是我们跟踪的对象，每个目标有自己的状态，如大小、位置、速度。
        观测是指目标检测算法在当前帧检测出的目标，同样的，它也有大小、位置、速度等状态值。在这里，我们要建立目标与观测之间的对应关系。
        采用了多种线索来跟住目标，包括3D坐标，2D图像块，2D包围盒，以及通过深度学习得到的ROI特征。对于多目标跟踪，这里采用了多假设跟踪算法[3]，这种算法最早用于雷达数据的跟踪
    4、对红绿灯检测算法的描述：
        “红绿灯模块根据自身的位置查找地图，可以获得前方红绿灯的坐标位置。通过标定参数，可以将红绿灯从世界坐标系投影到图像坐标系，从而完成相机的自适应选择切换。选定相机后，在投影区域外选取一个较大的感兴趣区域，
        在其中运行红绿灯检测来获得精确的红绿灯框位置，并根据此红绿灯框的位置进行红绿灯的颜色识别，得到红绿灯当前的状态。得到单帧的红绿灯状态后，通过时序的滤波矫正算法进一步确认红绿灯的最终状态。
        我们提出的基于CNN的红绿灯的检测和识别算法具有极高的召回率和准确率，可以支持白天和夜晚的红绿灯检测识别。
        


